{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier Class\n",
    "class NaiveBayes:    \n",
    "    def prior(self, X, y):\n",
    "        # calculate prior probability P(y)\n",
    "        self.prior = (X.groupby(y).apply(lambda x: len(x)) / self.rows).to_numpy()\n",
    "        return self.prior\n",
    "    \n",
    "    def stats(self, X, y):\n",
    "        #calculate mean, variance for each column and convert to numpy array\n",
    "        self.mu = X.groupby(y).apply(np.mean).to_numpy()\n",
    "        self.sigma = X.groupby(y).apply(np.var).to_numpy()              \n",
    "        return self.mu, self.sigma\n",
    "    \n",
    "    def density(self, i, x):     \n",
    "        #calculate probability from gaussian density function (normally distributed)\n",
    "        mu = self.mu[i]\n",
    "        sigma = self.sigma[i]\n",
    "        n = np.exp((-1 / 2) * ((x - mu) ** 2) / (2 * sigma))\n",
    "        d = np.sqrt(2 * np.pi * sigma)\n",
    "        p = n / d\n",
    "        return p\n",
    "    \n",
    "    def posterior(self, x):\n",
    "        posteriors = []\n",
    "        # calculate posterior probability for each class\n",
    "        for i in range(self.count):\n",
    "            prior = np.log(self.prior[i]) ## use the log to make it more numerically stable\n",
    "            conditional = np.sum(np.log(self.density(i, x))) # use the log to make it more numerically stable\n",
    "            posterior = prior + conditional\n",
    "            posteriors.append(posterior)\n",
    "        # return class with highest posterior probability\n",
    "        return self.classes[np.argmax(posteriors)]     \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.count = len(self.classes)\n",
    "        self.feature_nums = X.shape[1]\n",
    "        self.rows = X.shape[0]\n",
    "        \n",
    "        self.stats(X, y)\n",
    "        self.prior(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = [self.posterior(f) for f in X.to_numpy()]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate = 0.01, epoch = 100000, fit_intercept = True, lam = 0.1, b = 0.5, func = \"grad\", isprint = False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.lam = lam\n",
    "        self.b = b\n",
    "        self.isprint = isprint\n",
    "        self.func = func\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def loss(self, y_h, y):\n",
    "        y = np.squeeze(np.asarray(y))\n",
    "        y_h = np.squeeze(np.asarray(y_h))\n",
    "        if self.func == \"grad\":\n",
    "            return -np.mean(y * np.log(y_h) + (1.0001 - y) * (np.log(1.0001 - y_h)))\n",
    "        elif self.func == \"l1\":\n",
    "            return -np.mean(y * np.log(y_h) + (1.0001 - y) * (np.log(1.0001 - y_h))) + (self.lam * (np.sum(self.theta)))\n",
    "        else:\n",
    "            return -np.mean(y * np.log(y_h) + (1.0001 - y) * (np.log(1.0001 - y_h))) + (self.lam * (np.sum(np.square(self.theta))))\n",
    "    \n",
    "    def add_intercept(self, X):\n",
    "        i = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((i, X), axis=1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.add_intercept(X)            \n",
    "        self.theta = np.zeros((X.shape[1], 1))\n",
    "        self.b = np.zeros((len(y), 1))\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            if self.func == \"grad\":\n",
    "                z = np.dot(X, self.theta)\n",
    "                y_h = self.sigmoid(z)\n",
    "                gradient = np.dot(X.T, (y_h - y)) / len(y)                \n",
    "            else:\n",
    "                z = np.dot(X, self.theta) + self.lam\n",
    "                y_h = self.sigmoid(z)\n",
    "                if self.func == \"l1\":\n",
    "                    gradient = np.dot(X.T, (y_h - y)) / len(y) + self.lam\n",
    "                else:\n",
    "                    gradient = np.dot(X.T, (y_h - y)) / len(y) + self.lam * self.theta            \n",
    "            self.theta = self.theta - (self.learning_rate * gradient)          \n",
    "            self.b = self.b - (self.learning_rate * np.sum(y_h - y))\n",
    "            loss = self.loss(y_h, y)\n",
    "            \n",
    "            if(self.isprint == True and i % self.epoch == 0):\n",
    "                print(self.func, \"Loss:\", loss)\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.add_intercept(X)    \n",
    "        return self.sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/banknote.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 4) (960,)\n",
      "(412, 4) (412,)\n"
     ]
    }
   ],
   "source": [
    "labelencoder=LabelEncoder()\n",
    "for column in df.columns:\n",
    "    df[column] = labelencoder.fit_transform(df[column])\n",
    "X = df.drop(['class'], axis=1)\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
    "y_train1 = np.asmatrix(y_train).T\n",
    "y_test1 = np.asmatrix(y_test).T\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 107 ms\n",
      "Logistic Regression with Gradient Descent Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.98      0.82       229\n",
      "           1       0.96      0.48      0.64       183\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       412\n",
      "   macro avg       0.83      0.73      0.73       412\n",
      "weighted avg       0.82      0.76      0.74       412\n",
      "\n",
      "Wall time: 118 ms\n",
      "Logistic Regression with L1 regularization Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.73       229\n",
      "           1       1.00      0.07      0.12       183\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       412\n",
      "   macro avg       0.79      0.53      0.43       412\n",
      "weighted avg       0.76      0.58      0.46       412\n",
      "\n",
      "Wall time: 126 ms\n",
      "Logistic Regression with L2 Regularization Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.97      0.86       229\n",
      "           1       0.95      0.65      0.77       183\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       412\n",
      "   macro avg       0.86      0.81      0.82       412\n",
      "weighted avg       0.85      0.83      0.82       412\n",
      "\n",
      "Wall time: 12 ms\n",
      "Naive Bayes Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89       229\n",
      "           1       0.89      0.81      0.85       183\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       412\n",
      "   macro avg       0.87      0.87      0.87       412\n",
      "weighted avg       0.87      0.87      0.87       412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrg = LogisticRegression(learning_rate=0.0001, epoch=1000,func=\"grad\")\n",
    "%time lrg.fit(X_train, y_train1)\n",
    "pg = lrg.predict(X_test)\n",
    "print(\"Logistic Regression with Gradient Descent Classifier report: \\n\\n\", classification_report(y_test1, pg))\n",
    "lrl1 = LogisticRegression(learning_rate=0.0001, epoch=1000,func=\"l1\")\n",
    "%time lrl1.fit(X_train, y_train1)\n",
    "pl1 = lrl1.predict(X_test)\n",
    "print(\"Logistic Regression with L1 regularization Classifier report: \\n\\n\", classification_report(y_test1, pl1))\n",
    "lrl2 = LogisticRegression(learning_rate=0.0001, epoch=1000,func=\"l2\")\n",
    "%time lrl2.fit(X_train, y_train1)\n",
    "pl2 = lrl2.predict(X_test)\n",
    "print(\"Logistic Regression with L2 Regularization Classifier report: \\n\\n\", classification_report(y_test1, pl2))\n",
    "nb = NaiveBayes()\n",
    "%time nb.fit(X_train, y_train)\n",
    "pnb = nb.predict(X_test)\n",
    "print(\"Naive Bayes Classifier report: \\n\\n\", classification_report(y_test, pnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab = pd.read_csv(\"data/diabetes.csv\")\n",
    "diab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 False\n",
       "Glucose                     False\n",
       "BloodPressure               False\n",
       "SkinThickness               False\n",
       "Insulin                     False\n",
       "BMI                         False\n",
       "DiabetesPedigreeFunction    False\n",
       "Age                         False\n",
       "Outcome                     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                   int64\n",
       "Glucose                       int64\n",
       "BloodPressure                 int64\n",
       "SkinThickness                 int64\n",
       "Insulin                       int64\n",
       "BMI                         float64\n",
       "DiabetesPedigreeFunction    float64\n",
       "Age                           int64\n",
       "Outcome                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  \\\n",
      "140            3       66             26              0        0   14   \n",
      "358           12       26             23             33       26  140   \n",
      "143           10       46             19              0        0  113   \n",
      "674            8       29             28              0        0  143   \n",
      "121            6       49             17             32        0  129   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  \n",
      "140                       136   34  \n",
      "358                       212   27  \n",
      "143                       140   21  \n",
      "674                       331   47  \n",
      "121                       129    3  \n"
     ]
    }
   ],
   "source": [
    "labelencoder=LabelEncoder()\n",
    "for column in diab.columns:\n",
    "    diab[column] = labelencoder.fit_transform(diab[column])\n",
    "X = diab.drop(['Outcome'], axis=1)\n",
    "y = diab['Outcome']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 5)\n",
    "print(X_train[:5])\n",
    "x_train = np.reshape(X_train.values, (-1, 1))\n",
    "yy_train = np.reshape(y_train.values, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 8) (614, 1)\n",
      "(154, 8) (154,)\n"
     ]
    }
   ],
   "source": [
    "#X_train = np.asmatrix(X_train)\n",
    "y_train1 = np.asmatrix(y_train).T\n",
    "#X_test = np.asmatrix(X_test)\n",
    "y_test1 = np.asmatrix(y_test).T\n",
    "print(X_train.shape, y_train1.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 83.8 ms\n",
      "Logistic Regression with Gradient Descent Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.87      0.79       100\n",
      "           1       0.63      0.41      0.49        54\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       154\n",
      "   macro avg       0.68      0.64      0.64       154\n",
      "weighted avg       0.70      0.71      0.69       154\n",
      "\n",
      "Wall time: 100 ms\n",
      "Logistic Regression with L1 regularization Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.89      0.79       100\n",
      "           1       0.62      0.33      0.43        54\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       154\n",
      "   macro avg       0.67      0.61      0.61       154\n",
      "weighted avg       0.68      0.69      0.67       154\n",
      "\n",
      "Wall time: 105 ms\n",
      "Logistic Regression with L2 Regularization Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.89      0.79       100\n",
      "           1       0.62      0.33      0.43        54\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       154\n",
      "   macro avg       0.67      0.61      0.61       154\n",
      "weighted avg       0.68      0.69      0.67       154\n",
      "\n",
      "Wall time: 9.98 ms\n",
      "Naive Bayes Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.83       100\n",
      "           1       0.77      0.44      0.56        54\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       154\n",
      "   macro avg       0.77      0.69      0.70       154\n",
      "weighted avg       0.76      0.76      0.74       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrg = LogisticRegression(learning_rate=0.0001, epoch=1000,func=\"grad\")\n",
    "%time lrg.fit(X_train, y_train1)\n",
    "pg = lrg.predict(X_test)\n",
    "print(\"Logistic Regression with Gradient Descent Classifier report: \\n\\n\", classification_report(y_test1, pg))\n",
    "lrl1 = LogisticRegression(learning_rate=0.0001, epoch=1000,func=\"l1\")\n",
    "%time lrl1.fit(X_train, y_train1)\n",
    "pl1 = lrl1.predict(X_test)\n",
    "print(\"Logistic Regression with L1 regularization Classifier report: \\n\\n\", classification_report(y_test1, pl1))\n",
    "lrl2 = LogisticRegression(learning_rate=0.0001, epoch=1000,func=\"l2\")\n",
    "%time lrl2.fit(X_train, y_train1)\n",
    "pl2 = lrl2.predict(X_test)\n",
    "print(\"Logistic Regression with L2 Regularization Classifier report: \\n\\n\", classification_report(y_test1, pl2))\n",
    "nb = NaiveBayes()\n",
    "%time nb.fit(X_train, y_train)\n",
    "pnb = nb.predict(X_test)\n",
    "print(\"Naive Bayes Classifier report: \\n\\n\", classification_report(y_test, pnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7311, 21) (7311,)\n",
      "(813, 21) (813,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/mushrooms.csv\")\n",
    "labelencoder=LabelEncoder()\n",
    "for column in df.columns:\n",
    "    df[column] = labelencoder.fit_transform(df[column])\n",
    "df = df.drop([\"veil-type\"],axis=1)\n",
    "X = df.drop(['class'], axis=1)\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\n",
    "y_train1 = np.asmatrix(y_train).T\n",
    "y_test1 = np.asmatrix(y_test).T\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 423 ms\n",
      "Logistic Regression with Gradient Descent Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.96      0.82       433\n",
      "           1       0.92      0.56      0.70       380\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       813\n",
      "   macro avg       0.82      0.76      0.76       813\n",
      "weighted avg       0.81      0.77      0.76       813\n",
      "\n",
      "Wall time: 394 ms\n",
      "Logistic Regression with L1 regularization Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.98      0.79       433\n",
      "           1       0.95      0.44      0.60       380\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       813\n",
      "   macro avg       0.81      0.71      0.70       813\n",
      "weighted avg       0.80      0.73      0.70       813\n",
      "\n",
      "Wall time: 424 ms\n",
      "Logistic Regression with L2 Regularization Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.98      0.80       433\n",
      "           1       0.96      0.46      0.62       380\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       813\n",
      "   macro avg       0.82      0.72      0.71       813\n",
      "weighted avg       0.81      0.74      0.72       813\n",
      "\n",
      "Wall time: 12.5 ms\n",
      "Naive Bayes Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.50      0.65       433\n",
      "           1       0.63      0.96      0.76       380\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       813\n",
      "   macro avg       0.78      0.73      0.71       813\n",
      "weighted avg       0.79      0.72      0.70       813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrg = LogisticRegression(learning_rate=0.0001, epoch=1000,func=\"grad\")\n",
    "%time lrg.fit(X_train, y_train1)\n",
    "pg = lrg.predict(X_test)\n",
    "print(\"Logistic Regression with Gradient Descent Classifier report: \\n\\n\", classification_report(y_test1, pg))\n",
    "lrl1 = LogisticRegression(learning_rate=0.0001, epoch=1000,func=\"l1\")\n",
    "%time lrl1.fit(X_train, y_train1)\n",
    "pl1 = lrl1.predict(X_test)\n",
    "print(\"Logistic Regression with L1 regularization Classifier report: \\n\\n\", classification_report(y_test1, pl1))\n",
    "lrl2 = LogisticRegression(learning_rate=0.0001, epoch=1000,func=\"l2\")\n",
    "%time lrl2.fit(X_train, y_train1)\n",
    "pl2 = lrl2.predict(X_test)\n",
    "print(\"Logistic Regression with L2 Regularization Classifier report: \\n\\n\", classification_report(y_test1, pl2))\n",
    "nb1 = NaiveBayes()\n",
    "%time nb1.fit(X_train, y_train)\n",
    "pnb = nb1.predict(X_test)\n",
    "print(\"Naive Bayes Classifier report: \\n\\n\", classification_report(y_test, pnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.90</td>\n",
       "      <td>7</td>\n",
       "      <td>286</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.93</td>\n",
       "      <td>4</td>\n",
       "      <td>249</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>132</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>accounting</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.71</td>\n",
       "      <td>4</td>\n",
       "      <td>151</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5</td>\n",
       "      <td>163</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.10             0.90               7                   286   \n",
       "1                0.89             0.93               4                   249   \n",
       "2                0.38             0.50               2                   132   \n",
       "3                0.95             0.71               4                   151   \n",
       "4                0.84             0.84               5                   163   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years       sales  \\\n",
       "0                   4              0     1                      0       sales   \n",
       "1                   3              0     0                      0       sales   \n",
       "2                   3              0     1                      0  accounting   \n",
       "3                   4              0     0                      0       sales   \n",
       "4                   3              0     0                      0   technical   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1     low  \n",
       "2     low  \n",
       "3  medium  \n",
       "4     low  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/hrdata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10114, 9) (10114,)\n",
      "(1124, 9) (1124,)\n"
     ]
    }
   ],
   "source": [
    "labelencoder=LabelEncoder()\n",
    "for column in df.columns:\n",
    "    df[column] = labelencoder.fit_transform(df[column])\n",
    "X = df.drop(['left'], axis=1)\n",
    "y = df['left']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\n",
    "y_train1 = np.asmatrix(y_train).T\n",
    "y_test1 = np.asmatrix(y_test).T\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 339 ms\n",
      "Logistic Regression with Gradient Descent Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.98      0.88       833\n",
      "           1       0.84      0.26      0.40       291\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1124\n",
      "   macro avg       0.81      0.62      0.64      1124\n",
      "weighted avg       0.80      0.80      0.75      1124\n",
      "\n",
      "Wall time: 304 ms\n",
      "Logistic Regression with L1 regularization Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       833\n",
      "           1       0.96      0.26      0.41       291\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      1124\n",
      "   macro avg       0.88      0.63      0.64      1124\n",
      "weighted avg       0.84      0.81      0.76      1124\n",
      "\n",
      "Wall time: 314 ms\n",
      "Logistic Regression with L2 Regularization Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       833\n",
      "           1       0.96      0.26      0.41       291\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      1124\n",
      "   macro avg       0.88      0.63      0.64      1124\n",
      "weighted avg       0.84      0.81      0.76      1124\n",
      "\n",
      "Wall time: 15.9 ms\n",
      "Naive Bayes Classifier report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88       833\n",
      "           1       0.67      0.65      0.66       291\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1124\n",
      "   macro avg       0.78      0.77      0.77      1124\n",
      "weighted avg       0.83      0.83      0.83      1124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrg = LogisticRegression(learning_rate=0.00001, epoch=1000,func=\"grad\")\n",
    "%time lrg.fit(X_train, y_train1)\n",
    "pg = lrg.predict(X_test)\n",
    "print(\"Logistic Regression with Gradient Descent Classifier report: \\n\\n\", classification_report(y_test1, pg))\n",
    "lrl1 = LogisticRegression(learning_rate=0.00001, epoch=1000,func=\"l1\")\n",
    "%time lrl1.fit(X_train, y_train1)\n",
    "pl1 = lrl1.predict(X_test)\n",
    "print(\"Logistic Regression with L1 regularization Classifier report: \\n\\n\", classification_report(y_test1, pl1))\n",
    "lrl2 = LogisticRegression(learning_rate=0.00001, epoch=1000,func=\"l2\")\n",
    "%time lrl2.fit(X_train, y_train1)\n",
    "pl2 = lrl2.predict(X_test)\n",
    "print(\"Logistic Regression with L2 Regularization Classifier report: \\n\\n\", classification_report(y_test1, pl2))\n",
    "nb1 = NaiveBayes()\n",
    "%time nb1.fit(X_train, y_train)\n",
    "pnb = nb1.predict(X_test)\n",
    "print(\"Naive Bayes Classifier report: \\n\\n\", classification_report(y_test, pnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
